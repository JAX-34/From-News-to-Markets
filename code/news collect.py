import os
import time
import re
import json
import math
from datetime import datetime, timedelta
import requests
import pandas as pd
import numpy as np

# key
API_KEY   = "personal key"#
STOCK_CSV = "test_stock_data.csv"
LANG      = "en"
COUNTRY   = "us"
OUT_DIR   = "news_out"
MAX_PER_PAGE = 100
PAGES_PER_WINDOW = 10


# Keyword mapping
QUERY_MAP = {
    "TSLA": '"Tesla" OR "Elon Musk"',
    "AAPL": '"Apple Inc." OR "Apple" OR "Apple stock"',
    "MSFT": '"Microsoft" OR "MSFT" OR "Azure"',
    "GOOG": '"Google" OR "Alphabet Inc." OR "Waymo"',
    "AMZN": '"Amazon.com" OR "Amazon" OR "AWS"',
    "META": '"Meta" OR "Facebook" OR "Instagram" OR "WhatsApp"',
    "NVDA": '"NVIDIA" OR "NVDA" OR "GPU"',
    "NIO" : '"NIO" OR "è”šæ¥" OR "è”šæ¥æ±½è½¦"',
    "RIVN": '"Rivian"'
}

#  FinBERT (financial sentiment)
from transformers import BertTokenizer, BertForSequenceClassification
import torch
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')
model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone').to(DEVICE)
model.eval()

def finbert_score(text: str):

    text = (text or "").strip()
    if not text:
        return "neutral", 0
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256).to(DEVICE)
    with torch.no_grad():
        logits = model(**inputs).logits
        probs = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()
    idx = probs.argmax()
    label_map = {0: "positive", 1: "neutral", 2: "negative"}  # æ³¨æ„ï¼šè¯¥æ¨¡å‹ç±»ç›®é¡ºåºæ˜¯ positive, neutral, negative
    label = label_map[idx]
    score = {"positive": 1, "neutral": 0, "negative": -1}[label]
    return label, score

# ======== å·¥å…·å‡½æ•° ========
def month_spans(start_date: pd.Timestamp, end_date: pd.Timestamp):
    """æŠŠæ•´ä½“æ—¥æœŸæ‹†æˆè‹¥å¹²è‡ªç„¶æœˆåŒºé—´ï¼ˆå­—ç¬¦ä¸² YYYY-MM-DDï¼‰"""
    spans = []
    cur = pd.Timestamp(year=start_date.year, month=start_date.month, day=1)
    end_month_start = pd.Timestamp(year=end_date.year, month=end_date.month, day=1)
    while cur <= end_month_start:
        if cur.month == 12:
            month_end = pd.Timestamp(year=cur.year, month=12, day=31)
        else:
            month_end = (cur + pd.offsets.MonthBegin(1)) - pd.Timedelta(days=1)
        # è£å‰ªåˆ°ç”¨æˆ·èŒƒå›´
        s = max(cur, start_date)
        e = min(month_end, end_date)
        spans.append((s.strftime("%Y-%m-%d"), e.strftime("%Y-%m-%d")))
        cur = cur + pd.offsets.MonthBegin(1)
    return spans

def clean_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    # å»ä¸å¯è§/éASCIIï¼ˆExcelå¸¸è§â€œéˆ¥â€ç±»ï¼‰
    s = re.sub(r'[^\x00-\x7F]+', ' ', s)
    # å‹ç¼©å¤šç©ºæ ¼
    s = re.sub(r'\s+', ' ', s).strip()
    return s

def gnews_search(query, start_date, end_date, page=1, max_results=MAX_PER_PAGE):
    """è°ƒç”¨ GNews ä¼šå‘˜æ¥å£ï¼ˆæ”¯æŒ from/to/pageï¼‰"""
    url = "https://gnews.io/api/v4/search"
    params = {
        "q": query,
        "lang": LANG,
        "country": COUNTRY,
        "from": start_date,
        "to": end_date,
        "max": max_results,
        "page": page,
        "apikey": API_KEY
    }
    resp = requests.get(url, params=params, timeout=30)
    if resp.status_code != 200:
        print(f"âŒ GNews {resp.status_code}: {resp.text[:200]}")
        return None
    return resp.json()

def collect_company_month(ticker, query, start_date, end_date):
    """æŠ“å–å•å…¬å¸æŸä¸ªæœˆä»½æ‰€æœ‰é¡µï¼Œè¿”å› list[dict]"""
    all_items = []
    for p in range(1, PAGES_PER_WINDOW + 1):
        data = gnews_search(query, start_date, end_date, page=p)
        if not data:
            break
        arts = data.get("articles", [])
        if not arts:
            break
        for a in arts:
            all_items.append({
                "date": (a.get("publishedAt","")[:10] or ""),
                "ticker": ticker,
                "title": clean_text(a.get("title","")),
                "description": clean_text(a.get("description","")),
                "content": clean_text(a.get("content","")),
                "source": (a.get("source",{}) or {}).get("name",""),
                "url": a.get("url",""),
                "query": query
            })
        # ç®€å•é™é¢‘
        time.sleep(0.6)
    return all_items

# ======== ä¸»æµç¨‹ ========
def main():
    os.makedirs(OUT_DIR, exist_ok=True)

    # 1) è¯»å–è‚¡ç¥¨è®­ç»ƒé›†ï¼Œç¡®å®šæ—¥æœŸèŒƒå›´ä¸ Ticker åˆ—
    stock = pd.read_csv(STOCK_CSV)
    if "Date" not in stock.columns:
        raise ValueError("è‚¡ç¥¨CSVç¼ºå°‘ Date åˆ—")
    stock["Date"] = pd.to_datetime(stock["Date"])
    date_min = stock["Date"].min()
    date_max = stock["Date"].max()
    tickers = [c for c in stock.columns if c != "Date"]
    print(f"âœ… è¯»å–è‚¡ç¥¨è¡¨ï¼š{STOCK_CSV}ï¼Œæ—¥æœŸèŒƒå›´ {date_min.date()} ~ {date_max.date()}ï¼Œè‚¡ç¥¨ï¼š{tickers}")

    # 2) é€å…¬å¸é€æœˆä»½æŠ“æ–°é—»
    raw_rows = []
    spans = month_spans(date_min, date_max)
    for tkr in tickers:
        query = QUERY_MAP.get(tkr, f'"{tkr}" stock OR earnings')
        print(f"\n==== {tkr} | query: {query} ====")
        for s, e in spans:
            print(f"  âŒ› æŠ“å– {s} ~ {e}")
            items = collect_company_month(tkr, query, s, e)
            print(f"    â• æœ¬æœˆæŠ“åˆ° {len(items)} æ¡")
            raw_rows.extend(items)

    if not raw_rows:
        print("âš ï¸ æœªæŠ“åˆ°ä»»ä½•æ–°é—»ï¼Œè¯·æ£€æŸ¥ API Key / é…é¢ / å‚æ•°")
        return

    raw_df = pd.DataFrame(raw_rows)
    raw_df = raw_df.drop_duplicates(subset=["date","ticker","title"]).reset_index(drop=True)

    # 3) è¿è¡Œ FinBERT æƒ…ç»ª
    print("\nğŸ§  FinBERT æƒ…ç»ªåˆ†æä¸­â€¦")
    labels = []
    scores = []
    for i, row in raw_df.iterrows():
        text = row["title"] if row["title"] else row["description"]
        label, score = finbert_score(text)
        labels.append(label)
        scores.append(score)
        if (i+1) % 200 == 0:
            print(f"  å·²å¤„ç† {i+1}/{len(raw_df)}")
    raw_df["sentiment_label"] = labels
    raw_df["sentiment_score"] = scores

    # 4) ä¿å­˜é€æ¡æ–°é—»æ˜ç»†
    year_tag = f"{date_min.year}" if date_min.year == date_max.year else f"{date_min.year}-{date_max.year}"
    raw_path = os.path.join(OUT_DIR, f"news_raw_{year_tag}.csv")
    raw_df.to_csv(raw_path, index=False, encoding="utf-8-sig")
    print(f"âœ… æ˜ç»†å·²ä¿å­˜ï¼š{raw_path}ï¼ˆ{len(raw_df)}æ¡ï¼‰")

    # 5) èšåˆæˆâ€œæ—¥Ã—è‚¡ç¥¨â€çš„å®½è¡¨ï¼Œä¸è‚¡ç¥¨æ—¥æœŸå¯¹é½
    raw_df["date"] = pd.to_datetime(raw_df["date"], errors="coerce")
    daily = (
        raw_df.groupby(["date","ticker"])
              .agg(avg_sentiment=("sentiment_score","mean"),
                   news_count=("sentiment_score","count"))
              .reset_index()
    )

    # ç”Ÿæˆä¸è‚¡ç¥¨è¡¨åŒæ ·çš„æ—¥æœŸç´¢å¼•
    all_days = stock[["Date"]].drop_duplicates().sort_values("Date")
    # pivot å¹³å‡æƒ…ç»ª
    mat_sent = daily.pivot(index="date", columns="ticker", values="avg_sentiment").reindex(all_days["Date"]).reset_index()
    mat_sent.rename(columns={"date":"Date"}, inplace=True)
    # ç¼ºå¤±å¡« 0ï¼ˆå½“å¤©æ— æ–°é—»ï¼‰
    for t in tickers:
        if t not in mat_sent.columns:
            mat_sent[t] = 0.0
    mat_sent = mat_sent[["Date"] + tickers].fillna(0.0)

    # pivot æ–°é—»æ¡æ•°
    mat_cnt = daily.pivot(index="date", columns="ticker", values="news_count").reindex(all_days["Date"]).reset_index()
    mat_cnt.rename(columns={"date":"Date"}, inplace=True)
    for t in tickers:
        if t not in mat_cnt.columns:
            mat_cnt[t] = 0
    mat_cnt = mat_cnt[["Date"] + tickers].fillna(0).astype({t:int for t in tickers})

    # 6) ä¿å­˜å®½è¡¨ï¼ˆä¸ä½ è‚¡ç¥¨è¡¨ç»“æ„ä¸€è‡´ï¼‰
    sent_path = os.path.join(OUT_DIR, "news_daily_sentiment.csv")
    cnt_path  = os.path.join(OUT_DIR, "news_daily_count.csv")
    mat_sent.to_csv(sent_path, index=False, encoding="utf-8-sig")
    mat_cnt.to_csv(cnt_path,  index=False, encoding="utf-8-sig")

    print(f"âœ… æ—¥å‡æƒ…ç»ªçŸ©é˜µï¼š{sent_path}  å½¢çŠ¶={mat_sent.shape}")
    print(f"âœ… å½“æ—¥æ–°é—»æ¡æ•°ï¼š{cnt_path}   å½¢çŠ¶={mat_cnt.shape}")
    print("\nğŸ¯ ç°åœ¨ä½ å·²ç»æ‹¥æœ‰ï¼š")
    print("   1) ä¸è‚¡ç¥¨è¡¨åŒç»“æ„çš„â€œæ—¥å‡æƒ…ç»ªçŸ©é˜µâ€ï¼ˆå¯ç›´æ¥å¹¶åˆ—æ‹¼æ¥ï¼‰")
    print("   2) å½“æ—¥æ–°é—»æ¡æ•°çŸ©é˜µï¼ˆå¯ä½œæƒé‡æˆ–ç‰¹å¾ï¼‰")
    print("   3) å…¨é‡é€æ¡æ–°é—»æ˜ç»†ï¼ˆå¯å¤æ ¸ä¸æº¯æºï¼‰")

if __name__ == "__main__":
    main()
